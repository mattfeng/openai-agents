{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattfeng/envs/deep/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.python.framework import dtypes, ops\n",
    "from tensorflow.python.ops import array_ops, control_flow_ops, embedding_ops, math_ops, nn_ops, variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "H = 8 # number of hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # Discount factor for reward\n",
    "decay_rate = 0.99 # Decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # Resume from previous checkpoint\n",
    "\n",
    "policy_bs = 3 # Batch size when training policy\n",
    "model_bs = 3 # Batch size when training model\n",
    "\n",
    "# Model initialization\n",
    "D = 4 # Input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattfeng/envs/deep/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None, 4], name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "a1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(a1, W2)\n",
    "proba = tf.nn.sigmoid(score) # Probability of taking action \"1\"\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "# The action that was performed, as a float\n",
    "input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "# How much reward offset by a baseline did we get after taking `this` action?\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
    "W2grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "batch_grad = [W1grad, W2grad]\n",
    "\n",
    "# Define loss function\n",
    "loglik = tf.log(input_y * (input_y - proba) +      # if input_y is 1\n",
    "                (1 - input_y) * (input_y + proba)) # if input_y is 0\n",
    "loss = -tf.reduce_mean(loglik * advantages)\n",
    "\n",
    "# Compute the gradient of the loss w.r.t the parameters\n",
    "new_grads = tf.gradients(loss, tvars)\n",
    "\n",
    "# Don't apply the gradients right away, in case we want to manipulate them in our training routine\n",
    "update_grads = adam.apply_gradients(zip(batch_grad, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Model Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "# Model \"encoder\": takes the previous observation + action and tries to find a rich embedding\n",
    "prev_state = tf.placeholder(tf.float32, [None, 5], name=\"prev_state\")\n",
    "W1_model = tf.get_variable(\"W1_model\", shape=[5, mH],\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1_model = tf.Variable(tf.zeros([mH]), name=\"B1_model\")\n",
    "A1_model = tf.nn.relu(tf.matmul(prev_state, W1_model) + B1_model)\n",
    "W2_model = tf.get_variable(\"W2_model\", shape=[mH, mH],\n",
    "                           initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2_model = tf.Variable(tf.zeros([mH]), name=\"B2_model\")\n",
    "A2_model = tf.nn.relu(tf.matmul(A1_model, W2_model) + B2_model)\n",
    "\n",
    "# Final layer to predict output\n",
    "wO = tf.get_variable(\"wO\", shape=[mH, 4],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "bO = tf.Variable(tf.zeros([4]), name=\"bO\")\n",
    "\n",
    "# Final layer to predict reward\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "bR = tf.Variable(tf.zeros([1]), name=\"bR\")\n",
    "\n",
    "# Final layer to predict if done\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "bD = tf.Variable(tf.ones([1]), name=\"bD\")\n",
    "\n",
    "pred_obs = tf.add(tf.matmul(A2_model, wO), bO, name=\"predicted_obs\")\n",
    "pred_reward = tf.add(tf.matmul(A2_model, wR), bR, name=\"predicted_reward\")\n",
    "pred_done = tf.sigmoid(tf.matmul(A2_model, wD) + bD, name=\"predicted_done\")\n",
    "\n",
    "true_obs = tf.placeholder(tf.float32, [None, 4], name=\"true_obs\")\n",
    "true_reward = tf.placeholder(tf.float32, [None, 1], name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32, [None, 1], name=\"true_done\")\n",
    "\n",
    "predicted_state = tf.concat([pred_obs, pred_reward, pred_done], axis=1)\n",
    "\n",
    "# Define losses for Model Network\n",
    "obs_loss = tf.square(true_obs - pred_obs)\n",
    "reward_loss = tf.square(true_reward - pred_reward)\n",
    "done_loss = tf.multiply(pred_done, true_done) + tf.multiply(1 - pred_done, 1 - true_done)\n",
    "# Make sure to not have a typo here (done_less -> fail)\n",
    "done_loss = -tf.log(done_loss)\n",
    "model_loss = tf.reduce_mean(obs_loss + done_loss + reward_loss)\n",
    "\n",
    "adam_model = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "update_model = adam_model.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reset_grad_buffer(buffer):\n",
    "    for ix, grad in enumerate(buffer):\n",
    "        buffer[ix] = grad * 0\n",
    "    return buffer\n",
    "\n",
    "def discount_rewards(rewards):\n",
    "    discounted_r = np.zeros_like(rewards)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        running_add = running_add * gamma + rewards[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "# Use our model to produce a new state when given a previous observation and action\n",
    "# Actually, we pass in the entire history of observations (shape: (1, 4)) to this function\n",
    "def step_model(sess, history, action):\n",
    "    prev_state_tofeed = np.reshape(np.hstack([history[-1][0],\n",
    "                                                np.array(action)]), [1, 5])\n",
    "    prediction = sess.run([predicted_state], feed_dict={prev_state: prev_state_tofeed})\n",
    "    reward = prediction[0][:, 4][0]\n",
    "    obs = prediction[0][:, 0:4]\n",
    "    obs[:, 0] = np.clip(obs[:, 0], -2.4, 2.4)\n",
    "    obs[:, 2] = np.clip(obs[:, 2], -0.4, 0.4)\n",
    "    done_pred = np.clip(prediction[0][:, 5], 0, 1)[0]\n",
    "#     print(\"predicted reward:\", reward)\n",
    "    if done_pred > 0.1 or len(history) >= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "#     print(\"model output: obs: {} \\t reward: {} \\t done: {}\".format(str(obs), str(reward), str(done)))\n",
    "    return obs, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Training the Policy and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./cartpole-model/cartpole-model\n",
      ">>> \n",
      ">>> \n",
      ">>> \n",
      ">>> \n",
      ">>> \n",
      ">>> \n",
      ">>> \n",
      ">>> n\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './cartpole-model/cartpole-model')\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        obs = np.reshape(obs, [1, 4])\n",
    "        env.render()\n",
    "        action_proba = sess.run(proba, feed_dict={observations: obs})\n",
    "        action = 1 if np.random.uniform() < action_proba else 0\n",
    "        \n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            cont = input('>>> ')\n",
    "            if len(cont) > 0 and cont[0].lower() == 'n':\n",
    "                break\n",
    "            obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
